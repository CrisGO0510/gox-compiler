## üöÄ Modo de uso

Con la extensi√≥n implementada, solo necesitas crear tu archivo `.gox` junto con el archivo `lexer.py`, que se encuentra en el repositorio. Aseg√∫rate de que ambos archivos est√©n en la misma carpeta antes de ejecutar.

### üîπ Ejemplo de c√≥digo GOX

Antes de ejecutar, aseg√∫rate de tener un archivo `.gox` con c√≥digo v√°lido. Aqu√≠ tienes un ejemplo:

```gox
var x = 10;
print(x);
```

### üîπ Ejecuci√≥n del c√≥digo

Puedes ejecutar el archivo `.gox` con la extensi√≥n de VS Code o directamente desde la terminal.

#### 1Ô∏è‚É£ Desde VS Code (usando la extensi√≥n)

Al abrir el archivo en VS Code, selecciona la opci√≥n de ejecuci√≥n disponible en la barra superior:

![Ejecuci√≥n del archivo GOX](/docs/images/run-gox-file.png)

#### 2Ô∏è‚É£ Desde la terminal (manualmente con Python)

Si prefieres ejecutarlo manualmente, usa el siguiente comando:

```sh
python lexer.py archivo.gox
```

Esto abrir√° la terminal con el **analizador l√©xico (Lexer)**, el cual se encargar√° de descomponer el c√≥digo en tokens.

---

## üîç **Lexer: Tokenizaci√≥n del c√≥digo**

El **Lexer** se encarga de analizar el c√≥digo y convertirlo en una lista de tokens. La salida se divide en dos partes:

### ‚úÖ **Tokens generados**

```sh
Token(TIPO, VALOR, N¬∞ de L√≠nea)
```

Por ejemplo, si ejecutamos el c√≥digo GOX anterior, la salida podr√≠a ser:

```sh
Token(VAR, 'var', 1)
Token(ID, 'x', 1)
Token(ASSIGN, '=', 1)
Token(NUM, '10', 1)
Token(SEMICOLON, ';', 1)
Token(PRINT, 'print', 2)
Token(OPEN_PAREN, '(', 2)
Token(ID, 'x', 2)
Token(CLOSE_PAREN, ')', 2)
Token(SEMICOLON, ';', 2)
```

### ‚ùå **Captura de errores en el Lexer**

Si hay un error l√©xico en el c√≥digo, la terminal mostrar√° algo como:

```sh
15: Caracter ilegal '%'
N¬∞ de L√≠nea: ERROR
```

Esto indica que en la l√≠nea 15 se encontr√≥ un car√°cter no v√°lido (`%`).

---

## üèóÔ∏è **Parser: An√°lisis de la estructura del c√≥digo**

El siguiente paso en la ejecuci√≥n del c√≥digo ser√° el **Parser**, el cual analizar√° la estructura y sintaxis del c√≥digo GOX bas√°ndose en los tokens generados por el **Lexer**.
